"""
AIå®¢æˆ·ç«¯ - ç®¡ç†æœ¬åœ°Ollamaè¿æ¥
"""
import os
from typing import Dict, Any, Optional
from langchain_community.chat_models import ChatOllama


class HybridAIClient:
    def __init__(self, config: Dict):
        self.config = config
        self.mode = config.get('ai', {}).get('mode', 'local')
        self.local_config = config.get('ai', {}).get('local', {})
        
        print(f"ğŸ¤– AIå®¢æˆ·ç«¯åˆå§‹åŒ– - æ¨¡å¼: {self.mode}")
    
    def get_client_for_task(self, task_type: str = "default") -> ChatOllama:
        """è·å–AIå®¢æˆ·ç«¯"""
        # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        model = self.local_config.get('model', 'deepseek-r1:1.5b')
        
        if task_type in ["report_generation", "complex_analysis"]:
            # å¤æ‚ä»»åŠ¡å¯ä½¿ç”¨æ›´å¤§æ¨¡å‹
            if "qwen3:8b" in self._get_available_models():
                model = "qwen3:8b"
        
        print(f"   ä½¿ç”¨æ¨¡å‹: {model} ({task_type})")
        
        return ChatOllama(
            base_url=self.local_config.get('base_url', 'http://localhost:11434'),
            model=model,
            temperature=self.local_config.get('temperature', 0.1),
            timeout=30
        )
    
    def _get_available_models(self) -> list:
        """è·å–å¯ç”¨çš„æœ¬åœ°æ¨¡å‹"""
        try:
            import requests
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = [model['name'] for model in response.json()['models']]
                return models
        except:
            pass
        return []
    
    def get_client(self) -> ChatOllama:
        """è·å–é»˜è®¤å®¢æˆ·ç«¯"""
        return self.get_client_for_task("default")